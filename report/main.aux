\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{chambers2013event}
\citation{cheung2013probabilistic}
\citation{valls2014toward}
\citation{flekova2015personality}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\citation{schafer2007integrating}
\citation{groza2015information}
\citation{bamman2014bayesian}
\citation{chambers2013event}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Character roles}{3}{subsection.2.1}}
\citation{flekova2015personality}
\citation{mccrae1992introduction}
\citation{elson2010extracting}
\citation{kokkinakis2011character}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Character profiling}{4}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Character relations}{4}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Resources}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data}{4}{subsection.3.1}}
\newlabel{ssec:data}{{3.1}{4}{Data}{subsection.3.1}{}}
\newlabel{ssec:data@cref}{{[subsection][1][3]3.1}{4}}
\citation{schmid2013probabilistic}
\citation{baroni2009wacky}
\citation{mikolov2013distributed}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Information given per annotated book (abbreviated titles). In order of the columns, we have the total number of tokens in the book, the number known characters, number of job-labeled characters (total number of jobs identified as valid), number of gender-labeled characters.\relax }}{5}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:books}{{1}{5}{Information given per annotated book (abbreviated titles). In order of the columns, we have the total number of tokens in the book, the number known characters, number of job-labeled characters (total number of jobs identified as valid), number of gender-labeled characters.\relax }{table.caption.2}{}}
\newlabel{tab:books@cref}{{[table][1][]1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Tools}{5}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Window example, with w=4. Note, in this sentence all tokens are taken for consideration, whereas in practice we worked with sentences without stopwords, and more in some cases (e.g. only specific PoS tags). We also use words to determine the window. In practice, this could also be whole sentences instead.\relax }}{6}{figure.caption.3}}
\newlabel{fig:window}{{1}{6}{Window example, with w=4. Note, in this sentence all tokens are taken for consideration, whereas in practice we worked with sentences without stopwords, and more in some cases (e.g. only specific PoS tags). We also use words to determine the window. In practice, this could also be whole sentences instead.\relax }{figure.caption.3}{}}
\newlabel{fig:window@cref}{{[figure][1][]1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Extracting character metadata}{6}{section.4}}
\newlabel{sec:metadata}{{4}{6}{Extracting character metadata}{section.4}{}}
\newlabel{sec:metadata@cref}{{[section][4][]4}{6}}
\citation{blei2003latent}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Extraction by LDA}{7}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Metadata-specific predictors}{9}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Profession}{9}{subsubsection.4.2.1}}
\newlabel{sssec:profession}{{4.2.1}{9}{Profession}{subsubsection.4.2.1}{}}
\newlabel{sssec:profession@cref}{{[subsubsection][1][4,2]4.2.1}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Gender}{9}{subsubsection.4.2.2}}
\newlabel{sssec:gender}{{4.2.2}{9}{Gender}{subsubsection.4.2.2}{}}
\newlabel{sssec:gender@cref}{{[subsubsection][2][4,2]4.2.2}{9}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Computing job proximity scores for a given character.\relax }}{10}{algocf.1}}
\newlabel{alg:proximity}{{1}{10}{Profession}{algocf.1}{}}
\newlabel{alg:proximity@cref}{{[algorithm][1][]1}{10}}
\@writefile{toc}{\contentsline {paragraph}{Pronoun}{10}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Adjective}{10}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Immediate surroundings}{10}{section*.7}}
\citation{perkins2010textclass}
\@writefile{toc}{\contentsline {paragraph}{Character name}{11}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Sentiment}{11}{subsubsection.4.2.3}}
\citation{mikolov2013efficient}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Cosine distance for given word vectors using pre-trained word2vec model, with skip-gram architecture and dim=700.\relax }}{12}{table.caption.9}}
\newlabel{tab:word_sim}{{2}{12}{Cosine distance for given word vectors using pre-trained word2vec model, with skip-gram architecture and dim=700.\relax }{table.caption.9}{}}
\newlabel{tab:word_sim@cref}{{[table][2][]2}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{12}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Profession predictor}{12}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Count vs Proximity predictor scores on \textit  {Au Bonheur des Dames} characters. Points with score 1 are an exact match between two words. Characters are ordered by decreasing occurrence count. \textit  {Note}: plots for other books are generally similar.\relax }}{13}{figure.caption.10}}
\newlabel{fig:count_vs_prox}{{2}{13}{Count vs Proximity predictor scores on \textit {Au Bonheur des Dames} characters. Points with score 1 are an exact match between two words. Characters are ordered by decreasing occurrence count. \textit {Note}: plots for other books are generally similar.\relax }{figure.caption.10}{}}
\newlabel{fig:count_vs_prox@cref}{{[figure][2][]2}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Perfect match ratio for different ranks. The full character-set has 108 characters. The individual ratio is the ratio for only the given rank, and the cumulative is the ratio up until and including the given rank.\relax }}{13}{table.caption.11}}
\newlabel{tab:match_ratio}{{3}{13}{Perfect match ratio for different ranks. The full character-set has 108 characters. The individual ratio is the ratio for only the given rank, and the cumulative is the ratio up until and including the given rank.\relax }{table.caption.11}{}}
\newlabel{tab:match_ratio@cref}{{[table][3][]3}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Mean of all book and character scores grouped by 'rank' of prediction. Here the plots are of the `count', `proximity' and `aggregate' (first two combined) predictors. The bars represent the confidence interval for the point estimate.\relax }}{14}{figure.caption.12}}
\newlabel{fig:rank_cpa}{{3}{14}{Mean of all book and character scores grouped by 'rank' of prediction. Here the plots are of the `count', `proximity' and `aggregate' (first two combined) predictors. The bars represent the confidence interval for the point estimate.\relax }{figure.caption.12}{}}
\newlabel{fig:rank_cpa@cref}{{[figure][3][]3}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A comparison of the different parameters for the job predictor. `full' indicates the prediction was made based on observations from the whole book, whereas `exposition' focuses only on the $k$ first occurrences (here $\qopname  \relax m{max}(\frac  {1}{10}L_i, 10)$, where $L_i$ is the length of $c_i$'s document). `decreasing' indicates that the weight of the contributions of each occurrence decreases (by rate seen in \cref  {sec:metadata}) with occurrences later in $c_i$'s document.\relax }}{15}{figure.caption.13}}
\newlabel{fig:job_params}{{4}{15}{A comparison of the different parameters for the job predictor. `full' indicates the prediction was made based on observations from the whole book, whereas `exposition' focuses only on the $k$ first occurrences (here $\max (\frac {1}{10}L_i, 10)$, where $L_i$ is the length of $c_i$'s document). `decreasing' indicates that the weight of the contributions of each occurrence decreases (by rate seen in \cref {sec:metadata}) with occurrences later in $c_i$'s document.\relax }{figure.caption.13}{}}
\newlabel{fig:job_params@cref}{{[figure][4][]4}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Gender predictor}{15}{subsection.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Comparison of different features and weights used on the gender prediction task.`G' is for gender, `F' means we used the full data, as opposed to `S' (for solo), where we worked only with sentences where the character occurs alone. `NW' means the scores are non-weighted, and `W' means they are weighted, with weights approximated from a logistic regression. The column 'Known ratio' indicates the ratio of data points for which the predictor was able to make a guess.\relax }}{16}{table.caption.14}}
\newlabel{tab:gender_metrics}{{4}{16}{Comparison of different features and weights used on the gender prediction task.`G' is for gender, `F' means we used the full data, as opposed to `S' (for solo), where we worked only with sentences where the character occurs alone. `NW' means the scores are non-weighted, and `W' means they are weighted, with weights approximated from a logistic regression. The column 'Known ratio' indicates the ratio of data points for which the predictor was able to make a guess.\relax }{table.caption.14}{}}
\newlabel{tab:gender_metrics@cref}{{[table][4][]4}{16}}
\newlabel{fig:gender_dist}{{5a}{16}{Distribution of gender predictions and true labels.\relax }{figure.caption.15}{}}
\newlabel{fig:gender_dist@cref}{{[subfigure][1][5]5a}{16}}
\newlabel{sub@fig:gender_dist}{{a}{16}{Distribution of gender predictions and true labels.\relax }{figure.caption.15}{}}
\newlabel{sub@fig:gender_dist@cref}{{[subfigure][1][5]5a}{16}}
\newlabel{fig:gender_error}{{5b}{16}{TP, TN, FP and FN rates for the gender predictions (`f' is said to be positive).\relax }{figure.caption.15}{}}
\newlabel{fig:gender_error@cref}{{[subfigure][2][5]5b}{16}}
\newlabel{sub@fig:gender_error}{{b}{16}{TP, TN, FP and FN rates for the gender predictions (`f' is said to be positive).\relax }{figure.caption.15}{}}
\newlabel{sub@fig:gender_error@cref}{{[subfigure][2][5]5b}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Gender predictor metrics.\relax }}{16}{figure.caption.15}}
\newlabel{fig:gender}{{5}{16}{Gender predictor metrics.\relax }{figure.caption.15}{}}
\newlabel{fig:gender@cref}{{[figure][5][]5}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Mean proportions of positive, negative and neutral sentences for reduced character documents. Here the mean for all characters are shown.\relax }}{17}{figure.caption.16}}
\newlabel{fig:sent_prop}{{6}{17}{Mean proportions of positive, negative and neutral sentences for reduced character documents. Here the mean for all characters are shown.\relax }{figure.caption.16}{}}
\newlabel{fig:sent_prop@cref}{{[figure][6][]6}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Sentiment predictor}{17}{subsection.5.3}}
\citation{elson2010extracting}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Performance of the sentiment predictor on some main characters. A character is said to be positive if it has a higher ratio than the negative, and vice-versa.\relax }}{18}{table.caption.17}}
\newlabel{tab:full_vs_red}{{5}{18}{Performance of the sentiment predictor on some main characters. A character is said to be positive if it has a higher ratio than the negative, and vice-versa.\relax }{table.caption.17}{}}
\newlabel{tab:full_vs_red@cref}{{[table][5][]5}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{18}{section.6}}
\citation{bamman2014bayesian}
\citation{flekova2015personality}
\citation{*}
\bibstyle{plain}
\bibdata{bibliography}
\bibcite{bamman2014bayesian}{1}
\bibcite{baroni2009wacky}{2}
\bibcite{blei2003latent}{3}
\bibcite{chambers2013event}{4}
\bibcite{cheung2013probabilistic}{5}
\bibcite{elson2010extracting}{6}
\bibcite{flekova2015personality}{7}
\bibcite{groza2015information}{8}
\bibcite{kokkinakis2011character}{9}
\bibcite{mccrae1992introduction}{10}
\bibcite{mikolov2013efficient}{11}
\bibcite{mikolov2013distributed}{12}
\bibcite{perkins2010textclass}{13}
\bibcite{rochat2015character}{14}
\bibcite{schafer2007integrating}{15}
\bibcite{schmid2013probabilistic}{16}
\bibcite{valls2014toward}{17}
